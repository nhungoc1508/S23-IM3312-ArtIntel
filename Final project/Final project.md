# Final project documentation
# The Neural Mirror
The world through the eye of the machine

*By Ngoc Hoang & Fatima Nadeem*

<p align="center"><img src="https://github.com/nhungoc1508/S23-IM3312-ArtIntel/blob/main/Final%20project/poster.png" width=600/></p>

## Introduction
For the final project, we created a generative and interactive artwork called The Neural Mirror which shows a continuous stream of images generated by Stable Diffusion using webcam capture as the input guiding the process. The main underlying concept of the project is to portray our world through the eye of the machine, showing continuously shifting and morphing images that are a reflection of the audience as generated by machine learning algorithms.
The project includes two main technical components: a p5.js sketch that periodically captures frames from a webcam to serve as input to the image generation process as well as displays the output images, and a Flask server that utilizes the diffusers library to conduct image-to-image generation with Stable Diffusion.
The p5.js sketch is available [here](https://editor.p5js.org/nhungoc1508/sketches/LLdn6sNhg). The code for the Flask server is available [here](https://github.com/nhungoc1508/S23-IM3312-ArtIntel/blob/main/Final%20project/app.py).

## Ideation & main concept
### Creative ideation
Our project's goal is to use machine learning to produce an original and fascinating visual work that generates images in real-time based on input from a subject in front of the camera. The incredible "Uncanny Mirror" by Mario Klingemann, which employs machine learning to create strange and distorted portraits that cast doubt on our perception of reality, served as our source of inspiration for our project. Klingemann's work is a perfect illustration of the ongoing investigation into the relationship between creativity, technology, and art as well as how we might push the limits of what is possible to make with these instruments.

The Neural Mirror expands on this investigation by drawing inspirations from Klingemann's work to build up on our own ideas and methods. Our idea adds a new degree of improvisation and interaction to the creative process by using machine learning to generate visual output in response to real-time input. This produces a dynamic and captivating work of art that invites the viewer to have an active role in the process of creating the finished product. Each experience will eventually then be a distinctive moving visual adventure.

In addition to challenging the viewer's concept of identity and self-image, this project attempts to investigate the link between human and machine perception. We seek to highlight concerns about how machines perceive us and how they interpret and portray our identities. This exploration invites people to think about their own self-image and how it is formed. It also raises questions about how machines interpret and portray our identities. With this project, we intend to create a meaningful experience that encourages the viewer to think about their relationship with technology and how it shapes their understanding of the world. By merging the human and machine, we hope to push the limits of art and technology and challenge what we believe is achievable in terms of creative expression.

### Technical ideation
The main technical principle of this project returns to the concept of the Stable Diffusion latent space and how it, and similar generative models, learn from huge volumes of input data. We have seen that Stable Diffusion text-to-image generation works by using the encoded textual prompt as an input to denoise a random noisy image [[link](https://stable-diffusion-art.com/how-stable-diffusion-work/#Text-to-image)]. The image-to-image generation process works a bit differently, with the model using both the image and the prompt as inputs, where the original image is first encoded into the image latent space before the denoising is conducted on this latent representation [[link](https://stable-diffusion-art.com/how-stable-diffusion-work/#Image-to-image)]. This image of latent space is of particular interest to us.

The latent space of Stable Diffusion is defined as “a low-dimensional vector space where each point maps to an image” [[link](https://keras.io/examples/generative/random_walks_with_stable_diffusion/)]. The image latent space of Stable Diffusion is then a compact representation of the billions of images it was trained on, derived from the model’s own understanding of the relationships among the images. We as humans cannot explicitly design this space, as it is an immense and messy job, and instead, we let the machine learning algorithms learn all of it on their own by feeding them tons of data and hope that they will be able to extract some meaningful understandings of our visual world, solely by looking at enough images. Essentially, by letting models learn their own image latent space, we are giving them the total freedom to find patterns, establish categorizations, and build hierarchies from images of our world. We have little insight into what patterns were extracted by the models or how they organize their categorizations. Furthermore, since the latent space was constructed by compressing images into much smaller dimensions and retaining only the most essential components, we have no idea what information was discarded or retained.

This project is yet another attempt in unraveling this black box. By giving the model an input image without any accompanying text prompt, we are forcing the model to first project the image onto its image latent space and then conduct denoising on that latent representation without text prompt guidance. Since denoising is essentially a process where the model ‘guesses’ what the original image looks like based on a noisy latent representation, the output image (without text prompt), in a way, can reveal how the original image fits into the model’s own system of categorizing the visual world based on its training data, or how the model sees the image (hence our tagline, “the world through the eye of the machine”).

## Process
As mentioned earlier, this project includes two components: a p5.js sketch serving as the user interface, and a Flask server running Stable Diffusion code for image generation.

### Webcam frame capture
The p5.js sketch is an interface that includes a video capture via webcam. This sketch periodically takes a still capture of the webcam feed and sends the image to the Flask app server.

### Stable Diffusion images generation
The Flask server waits until it has received at least 2 frames from the p5.js sketch before processing them. As soon as it has two consecutive frames frame_i and frame_i+1, the Stable Diffusion model produces two corresponding latent representations of the frames, latent_0 and latent_1. These are the compact representations of the input images (i.e. webcam frames) as projected onto the model’s image latent space, and when projected back out into output images, essentially reflect how the input images fit into the model’s overarching understanding of our world.

Before denoising and decoding these latent representations, we conduct an additional step of latent interpolation, generating 5 more intermediate latent matrices that lie in between the two original images. Recall that this concept of frame interpolation (or taking equal steps between two frames) was previously explored and experimented with in Ngoc’s visual project.

So from two consecutive video frames received from the p5.js interface, the server now has 7 consecutive latent representations. Next, all these latent matrices undergo denoising and decoding to generate output images, which are then sent back to the p5.js sketch to display.

### p5.js display
The output images received from the server are displayed full-screen on the sketch. Since the server returns the output images in batches of 7, the p5.js sketch displays them one by one with a small delay in between (1-2 seconds) to create a progression of images. To make the transitions smoother, we also add a fading in - fading out effect for every pair of consecutive images. Additionally, as soon as the sketch receives a response from the server, it captures the next video frame to send to the server, continuing the image generation process.

## Reflections & Conclusion
Through this project, we were able to learn more about what technology can and cannot do, explore creative ways to use it and realize how quickly this industry is reforming. The implementations of some of the tools we used - diffusers, transformers, tenserflow.js - are being rapidly updated and this would often result in a two-day-old completely working code breaking down on the third day with incompatibility issues.  Moreover, the process of processing an input image, encoding into the latent space of a pre-trained model and decoding the same input image out and projecting it on screen is to our surprise way more memory intensive than expected. For the same running out of memory issues with every process, we had to move from building our code on Google Colab to a local machine to finally using Professor’s research machine through the server.

While we initially worked with low-resolution images and decided to upscale each image through another model after the post-processing after decoding from the latent space, our set-up would run out of memory with even a single image. To allow real-time use we then decided to leave the upscaling process and just present the low-resolution image outputs on the full screen. This also aligns well with the artistic nature of our project presentation that anyways displays blurred and surreal depictions. This choice also contributes to the overall enigmatic and uncanny quality of the finished product. The blurred and warped quality of the photographs might produce a feeling of disquiet or surrealism, leaving viewers to speculate and it heightens the overall experience by creating a sense of variability and unpredictability. The Neural Mirror project pushes the limits of traditional visual art and questions our perceptions of what digital artwork can be by purposefully leaving the images in a low-resolution state.
